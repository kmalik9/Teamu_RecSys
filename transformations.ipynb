{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54ca1455",
   "metadata": {},
   "source": [
    "### Overview\n",
    "##### In this file, we handle complex transformations and feature engineering from the processed BiqQuery datasets, and store the resulting training examples in Cloud Storage as TFRecords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c618bef1",
   "metadata": {},
   "source": [
    "### Retrieve datasets from BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1668bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "bigquery_client = bigquery.Client()\n",
    "\n",
    "#Write Query on BQ\n",
    "USERS_QUERY = \"SELECT * FROM `teamu-542ac.user_dataset.users`\"\n",
    "POSTS_QUERY = \"SELECT * FROM `teamu-542ac.post_dataset.posts`\"\n",
    "INTERACTIONS_QUERY = \"SELECT * FROM `teamu-542ac.interaction_dataset.user_post_interactions`\"\n",
    "\n",
    "#Run the queries and write results to a pandas data frames\n",
    "Query_Results = bigquery_client.query(USERS_QUERY)\n",
    "users_df = Query_Results.to_dataframe()\n",
    "\n",
    "Query_Results = bigquery_client.query(POSTS_QUERY)\n",
    "posts_df = Query_Results.to_dataframe()\n",
    "\n",
    "Query_Results = bigquery_client.query(INTERACTIONS_QUERY)\n",
    "interactions_df = Query_Results.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb818377",
   "metadata": {},
   "source": [
    "### NaN replacements and datetime type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8759f48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill any NaN values (users without collaborations) with an empty list\n",
    "users_df['project_titles'] = users_df['project_titles'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "# Fill any NaN values (posts without comments) with an empty list\n",
    "posts_df['comments'] = posts_df['comments'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "# Convert created_at timestamp to hour of day and day of week\n",
    "interactions_df['hour_of_day'] = pd.to_datetime(interactions_df['interaction_time']).dt.hour \n",
    "interactions_df['day_of_week'] = pd.to_datetime(interactions_df['interaction_time']).dt.weekday # 0 = Monday, 6 = Sunday\n",
    "\n",
    "# Replace None values in passions, project titles, and bio with empty strings\n",
    "users_df['passions'] = users_df['passions'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '')\n",
    "users_df['project_titles'] = users_df['project_titles'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '')\n",
    "users_df['bio'] = users_df['bio'].apply(lambda x: str(x) if x else '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243e645c",
   "metadata": {},
   "source": [
    "### Tokenize string data with BERT and pad token sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5a3d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BERT tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Function to tokenize text fields using the BERT tokenizer\n",
    "def tokenize_with_bert(texts, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizes input texts using the BERT tokenizer.\n",
    "    \n",
    "    Args:\n",
    "    texts (list of str): The input text data.\n",
    "    max_length (int): The maximum length of tokenized sequences.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing token ids and attention masks.\n",
    "    \"\"\"\n",
    "    tokens = bert_tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    return tokens\n",
    "\n",
    "# Tokenize passions, project_titles, and bio using BERT tokenizer\n",
    "passions_texts = users_df['passions'].tolist()\n",
    "project_titles_texts = users_df['project_titles'].tolist()\n",
    "bio_texts = users_df['bio'].tolist()\n",
    "\n",
    "# Generate tokens for each field with the maximum length set according to your requirements\n",
    "max_length_passions = 10\n",
    "max_length_project_titles = 10\n",
    "max_length_bio = 50\n",
    "\n",
    "passions_tokens = tokenize_with_bert(passions_texts, max_length=max_length_passions)\n",
    "project_titles_tokens = tokenize_with_bert(project_titles_texts, max_length=max_length_project_titles)\n",
    "bio_tokens = tokenize_with_bert(bio_texts, max_length=max_length_bio)\n",
    "\n",
    "# For padded sequences, use the 'input_ids' returned by the tokenizer\n",
    "passions_padded = passions_tokens['input_ids']\n",
    "project_titles_padded = project_titles_tokens['input_ids']\n",
    "bio_padded = bio_tokens['input_ids']\n",
    "\n",
    "### Processing Post Data with BERT ###\n",
    "\n",
    "# Get post title, description, and comments text data\n",
    "title_texts = posts_df['title'].tolist()\n",
    "description_texts = posts_df['description'].tolist()\n",
    "comments_texts = [\" \".join(comment_list) if isinstance(comment_list, list) else \"\" for comment_list in posts_df['comments']]\n",
    "\n",
    "# Generate tokens for post fields\n",
    "max_length_title = 10\n",
    "max_length_description = 100\n",
    "max_length_comments = 100\n",
    "\n",
    "title_tokens = tokenize_with_bert(title_texts, max_length=max_length_title)\n",
    "description_tokens = tokenize_with_bert(description_texts, max_length=max_length_description)\n",
    "comments_tokens = tokenize_with_bert(comments_texts, max_length=max_length_comments)\n",
    "\n",
    "# For padded sequences, use the 'input_ids' returned by the tokenizer\n",
    "title_padded = title_tokens['input_ids']\n",
    "description_padded = description_tokens['input_ids']\n",
    "comments_padded = comments_tokens['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b873d72",
   "metadata": {},
   "source": [
    "### Apply RBF transformation to user location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f6276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "from shapely.wkt import loads as load_wkt\n",
    "from shapely import wkb\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Ensure that lat_long is either Point object or default Point\n",
    "users_df['lat_long'] = users_df['lat_long'].apply(lambda wkb_hex: wkb.loads(bytes.fromhex(wkb_hex)) if pd.notnull(wkb_hex) else Point(1, 1))\n",
    "\n",
    "# Extract latitude and longitude from the Point objects\n",
    "users_df['latitude'] = users_df['lat_long'].apply(lambda loc: loc.y if loc else None)\n",
    "users_df['longitude'] = users_df['lat_long'].apply(lambda loc: loc.x if loc else None)\n",
    "\n",
    "# Convert latitude and longitude to H3 index for geospatial representation (optional)\n",
    "users_df['h3_location'] = users_df.apply(\n",
    "    lambda row: h3.latlng_to_cell(row['latitude'], row['longitude'], 8) if pd.notnull(row['latitude']) and pd.notnull(row['longitude']) else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 1: Apply Standard Scaling to Latitude and Longitude\n",
    "scaler = StandardScaler()\n",
    "users_df[['latitude', 'longitude']] = scaler.fit_transform(users_df[['latitude', 'longitude']])\n",
    "\n",
    "# Step 2: Apply Radial Basis Function (RBF) Transformation\n",
    "def rbf_transform(lat, lon, centers, gamma=0.1):\n",
    "    \"\"\"Apply an RBF transformation to latitude and longitude coordinates.\n",
    "    \n",
    "    Args:\n",
    "        lat (float): Latitude value.\n",
    "        lon (float): Longitude value.\n",
    "        centers (np.array): Centers for RBF, which could be sampled points within the dataset.\n",
    "        gamma (float): The gamma value that controls the spread of the RBF.\n",
    "        \n",
    "    Returns:\n",
    "        np.array: RBF-transformed features.\n",
    "    \"\"\"\n",
    "    coords = np.array([lat, lon])\n",
    "    distances = np.linalg.norm(centers - coords, axis=1)\n",
    "    return np.exp(-gamma * distances ** 2)\n",
    "\n",
    "# Step 3: Choose RBF Centers\n",
    "# We choose a few centers based on representative latitude and longitude values in the dataset\n",
    "num_centers = 5\n",
    "centers = users_df[['latitude', 'longitude']].sample(n=num_centers).to_numpy()\n",
    "\n",
    "# Step 4: Apply RBF Transformation to All Users\n",
    "users_df['h3_location'] = users_df.apply(\n",
    "    lambda row: rbf_transform(row['latitude'], row['longitude'], centers), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414bbed2",
   "metadata": {},
   "source": [
    "### Post numerical feature normalization and further NaN filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb87f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical features (vote count, upvote count, view count, post length, comment count, avg_time_viewed)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "posts_df[['post_length', 'vote_count', 'upvote_count', 'view_count', 'comment_count', 'avg_time_viewed']] = scaler.fit_transform(\n",
    "    posts_df[['post_length', 'vote_count', 'upvote_count', 'view_count', 'comment_count', 'avg_time_viewed']]\n",
    ")\n",
    "\n",
    "# Fill NaN values with 0 for users with no interactions\n",
    "users_df[['viewed_posts', 'upvoted_posts', 'commented_posts']] = users_df[['viewed_posts', 'upvoted_posts', 'commented_posts']].fillna(0)\n",
    "\n",
    "# Fill missing values in numerical columns with 0\n",
    "interactions_df[\"view_duration_secs\"] = interactions_df[\"view_duration_secs\"].fillna(0)\n",
    "interactions_df[\"comment_length\"] = interactions_df[\"comment_length\"].fillna(0)\n",
    "interactions_df[\"hour_of_day\"] = interactions_df[\"hour_of_day\"].fillna(0)\n",
    "interactions_df[\"day_of_week\"] = interactions_df[\"day_of_week\"].fillna(0)\n",
    "\n",
    "# Fill missing interaction types with a placeholder (e.g., 'unknown')\n",
    "interactions_df[\"interaction_type\"] = interactions_df[\"interaction_type\"].fillna('unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881b0a1c",
   "metadata": {},
   "source": [
    "### Making sure to assign padded embeddings to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff89ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df['passions'] = passions_padded\n",
    "users_df['project_titles'] = project_titles_padded\n",
    "users_df['bio'] = bio_padded\n",
    "\n",
    "posts_df['title'] = title_padded\n",
    "posts_df['description'] = description_padded\n",
    "posts_df['comments'] = comments_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b103cf",
   "metadata": {},
   "source": [
    "### Use tfrecorder library to convert dataframes to TFRecords and store in GCS buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e29049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tfrecorder\n",
    "\n",
    "users_df.tensorflow.to_tfr(\n",
    "    output_dir='/user_bucket',\n",
    "    project='teamu-542ac',\n",
    "    region='us-central1')\n",
    "\n",
    "posts_df.tensorflow.to_tfr(\n",
    "    output_dir='/post_bucket',\n",
    "    project='teamu-542ac',\n",
    "    region='us-central1')\n",
    "\n",
    "interactions_df.tensorflow.to_tfr(\n",
    "    output_dir='/interaction_bucket',\n",
    "    project='teamu-542ac',\n",
    "    region='us-central1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
