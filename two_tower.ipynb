{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2f97360",
   "metadata": {},
   "source": [
    "### Two Tower Model for learning user and post embeddings based on interaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390599f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d531e4",
   "metadata": {},
   "source": [
    "### Define user and post towers, as well as two tower model for learning embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3753ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "    def __init__(self, user_dataset):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Vocabulary size (replace with the actual size of your vocabularies)\n",
    "        location_vocab_size = 10000\n",
    "        \n",
    "        embedding_dim = 32  # Set the embedding dimension\n",
    "        \n",
    "        # String lookup layers to convert strings to integer indices\n",
    "        self.location_lookup = tf.keras.layers.StringLookup(max_tokens=location_vocab_size)\n",
    "    \n",
    "        # Projection layers for BERT embeddings\n",
    "        self.passions_projection = layers.Dense(embedding_dim, activation=\"relu\")\n",
    "        self.project_titles_projection = layers.Dense(embedding_dim, activation=\"relu\")\n",
    "        self.bio_projection = layers.Dense(embedding_dim, activation=\"relu\")\n",
    "\n",
    "        # Embedding location layer\n",
    "        self.location_embedding = layers.Embedding(input_dim=location_vocab_size, output_dim=embedding_dim)\n",
    "        \n",
    "        # Dense layers to combine features\n",
    "        self.dense_layers = tf.keras.Sequential([\n",
    "            layers.Dense(128, activation=\"relu\"),\n",
    "            layers.Dense(embedding_dim)\n",
    "        ])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # This is where you initialize all variables\n",
    "        self.built = True  # Mark the layer as built to prevent additional builds\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Inputs for textual data\n",
    "        passions_embedding = self.passions_projection(inputs['passions'])  # Projected to 32 dimensions\n",
    "        project_titles_embedding = self.project_titles_projection(inputs['project_titles'])\n",
    "        bio_embedding = self.bio_projection(inputs['bio'])\n",
    "        \n",
    "        # Convert string categorical data to integer indices\n",
    "        h3_location = inputs['h3_location']\n",
    "        \n",
    "        # Embedding lookups\n",
    "        location_embedding = self.location_embedding(h3_location)  # Now uses the integer indices\n",
    "        location_embedding = tf.reduce_mean(location_embedding, axis=1)  # Shape: (batch_size, embedding_dim)\n",
    "        \n",
    "        # Concatenate all embeddings\n",
    "        user_embedding = tf.concat([\n",
    "            tf.reduce_mean(passions_embedding, axis=1),  # Average the embeddings\n",
    "            tf.reduce_mean(project_titles_embedding, axis=1),\n",
    "            tf.reduce_mean(bio_embedding, axis=1),\n",
    "            location_embedding\n",
    "        ], axis=1)\n",
    "        \n",
    "        # Pass through dense layers to combine features\n",
    "        return self.dense_layers(user_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fac2edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostModel(tf.keras.Model):\n",
    "    def __init__(self, post_dataset):\n",
    "        super().__init__()\n",
    "        \n",
    "        embedding_dim = 32  # Set the embedding dimension\n",
    "\n",
    "        # BERT embedding layers projected to 32 dimensions\n",
    "        self.title_projection = layers.Dense(embedding_dim, activation=\"relu\")\n",
    "        self.description_projection = layers.Dense(embedding_dim, activation=\"relu\")\n",
    "        \n",
    "        # Dense layers to combine all post features\n",
    "        self.dense_layers = tf.keras.Sequential([\n",
    "            layers.Dense(128, activation=\"relu\"),\n",
    "            layers.Dense(embedding_dim)\n",
    "        ])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "\n",
    "        title_embedding = self.title_projection(inputs['title'])  # Projected to 32 dimensions\n",
    "        description_embedding = self.description_projection(inputs['description'])\n",
    "        \n",
    "        # Concatenate all embeddings\n",
    "        post_embedding = tf.concat([\n",
    "            tf.reduce_mean(title_embedding, axis=1),  # Average the embeddings\n",
    "            tf.reduce_mean(description_embedding, axis=1),\n",
    "        ], axis=1)\n",
    "        \n",
    "        # Pass through dense layers to combine features\n",
    "        return self.dense_layers(post_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3647fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Two-Tower Model combining User and Post Models\n",
    "class TwoTowerModel(tfrs.models.Model):\n",
    "\n",
    "    def __init__(self, user_model, post_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Save user and post towers\n",
    "        self.user_model = user_model\n",
    "        self.post_model = post_model\n",
    "        \n",
    "        # Retrieval task (metrics simplified)\n",
    "        self.task = tfrs.tasks.Retrieval()\n",
    "\n",
    "    \n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        # Extract user and post features from the input\n",
    "        user_inputs = {\n",
    "            'user_id': features['user_id'],\n",
    "            'passions': features['passions'],\n",
    "            'project_titles': features['project_titles'],\n",
    "            'bio': features['bio'],\n",
    "            'h3_location': features['h3_location'],\n",
    "        }\n",
    "    \n",
    "        post_inputs = {\n",
    "            'post_id': features['post_id'],\n",
    "            'title': features['title'],\n",
    "            'description': features['description']\n",
    "        }\n",
    "    \n",
    "        # Get user and post embeddings\n",
    "        user_embeddings = self.user_model(user_inputs)\n",
    "        post_embeddings = self.post_model(post_inputs)\n",
    "    \n",
    "        # Extract interaction weight\n",
    "        weight = features[\"weight\"]\n",
    "    \n",
    "        # Compute the retrieval loss\n",
    "        loss = self.task(user_embeddings, post_embeddings, compute_metrics=not training)\n",
    "    \n",
    "        # Scale the loss by the weight\n",
    "        weighted_loss = loss * tf.cast(weight, loss.dtype)\n",
    "    \n",
    "        # Aggregate the loss across the batch\n",
    "        batch_loss = tf.reduce_mean(weighted_loss)\n",
    "    \n",
    "        return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f8d2e4",
   "metadata": {},
   "source": [
    "### Load TFRecords from GCS buckets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f82fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# Define GCS paths for user, post, and interaction data\n",
    "USER_TFRECORD_PATH = 'gs://user_bucket/*.tfrecord'\n",
    "POST_TFRECORD_PATH = 'gs://post_bucket/*.tfrecord'\n",
    "INTERACTION_TFRECORD_PATH = 'gs://interaction_bucket/*.tfrecord'\n",
    "USER_EMBEDDINGS_OUTPUT_PATH = 'gs://vector_bucket/user_embeddings.npy'\n",
    "POST_EMBEDDINGS_OUTPUT_PATH = 'gs://vector_bucket/post_embeddings.npy'\n",
    "\n",
    "# Load data from TFRecord files\n",
    "def load_tfrecord_data(file_pattern, feature_description):\n",
    "    raw_dataset = tf.data.TFRecordDataset(tf.io.gfile.glob(file_pattern))\n",
    "    return raw_dataset.map(lambda x: tf.io.parse_single_example(x, feature_description))\n",
    "\n",
    "# Define the feature description dictionaries\n",
    "user_feature_description = {\n",
    "    'user_id': tf.io.FixedLenFeature([], tf.string),\n",
    "    'passions': tf.io.FixedLenFeature([], tf.string),\n",
    "    'project_titles': tf.io.FixedLenFeature([], tf.string),\n",
    "    'bio': tf.io.FixedLenFeature([], tf.string),\n",
    "    'h3_location': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "post_feature_description = {\n",
    "    'post_id': tf.io.FixedLenFeature([], tf.string),\n",
    "    'title': tf.io.FixedLenFeature([], tf.string),\n",
    "    'description': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "interaction_feature_description = {\n",
    "    'user_id': tf.io.FixedLenFeature([], tf.string),\n",
    "    'post_id': tf.io.FixedLenFeature([], tf.string),\n",
    "    'weight': tf.io.FixedLenFeature([], tf.float32),\n",
    "}\n",
    "\n",
    "# Load user, post, and interaction datasets\n",
    "user_dataset = load_tfrecord_data(USER_TFRECORD_PATH, user_feature_description)\n",
    "post_dataset = load_tfrecord_data(POST_TFRECORD_PATH, post_feature_description)\n",
    "interaction_dataset = load_tfrecord_data(INTERACTION_TFRECORD_PATH, interaction_feature_description)\n",
    "\n",
    "# Prepare the interaction data for training\n",
    "train_data = interaction_dataset.map(lambda x: {\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"post_id\": x[\"post_id\"],\n",
    "    \"weight\": x[\"weight\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5d4bc4",
   "metadata": {},
   "source": [
    "### Train the model and save embeddings to vector bucket in GCS for Vertex AI Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eff58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and compile the two-tower model\n",
    "user_model = UserModel(user_dataset)\n",
    "post_model = PostModel(post_dataset)\n",
    "two_tower_model = TwoTowerModel(user_model=user_model, post_model=post_model)\n",
    "two_tower_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "# Train the model\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 5\n",
    "\n",
    "two_tower_model.fit(train_data.batch(BATCH_SIZE), epochs=EPOCHS)\n",
    "\n",
    "# Function to save embeddings to Google Cloud Storage\n",
    "def save_embeddings(embeddings, output_path, name):\n",
    "    # Convert embeddings to tensors\n",
    "    embeddings_tensor = tf.convert_to_tensor(embeddings)\n",
    "    \n",
    "    # Save to GCS using Google Cloud Storage client\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(\"code_bucket\")\n",
    "    blob = bucket.blob(f\"{output_path}/{name}_embeddings.npy\")\n",
    "    blob.upload_from_string(tf.io.serialize_tensor(embeddings_tensor).numpy())\n",
    "\n",
    "# Generate and save user embeddings\n",
    "user_embeddings = []\n",
    "for user in user_dataset:\n",
    "    user_input = {\n",
    "        'passions': user['passions'],\n",
    "        'project_titles': user['project_titles'],\n",
    "        'bio': user['bio'],\n",
    "        'h3_location': user['h3_location']\n",
    "    }\n",
    "    user_embeddings.append(two_tower_model.user_model(user_input).numpy())\n",
    "save_embeddings(user_embeddings, USER_EMBEDDINGS_OUTPUT_PATH, \"user\")\n",
    "\n",
    "# Generate and save post embeddings\n",
    "post_embeddings = []\n",
    "for post in post_dataset:\n",
    "    post_input = {\n",
    "        'title': post['title'],\n",
    "        'description': post['description']\n",
    "    }\n",
    "    post_embeddings.append(two_tower_model.post_model(post_input).numpy())\n",
    "save_embeddings(post_embeddings, POST_EMBEDDINGS_OUTPUT_PATH, \"post\")\n",
    "\n",
    "# Save the trained model for inference or further usage\n",
    "two_tower_model.save(\"gs://model_bucket/models/ttm_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
