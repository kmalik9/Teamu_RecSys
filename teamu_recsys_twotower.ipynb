{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 927,
   "id": "a1f2066e-bdd0-47b1-8fed-fe3ff55cec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from supabase import create_client, Client\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import h3\n",
    "import datetime\n",
    "\n",
    "# Initialize Supabase client\n",
    "url = \"SUPABSE_URL\"\n",
    "key = \"SUPABASE_KEY\"\n",
    "\n",
    "supabase: Client = create_client(url, key)\n",
    "\n",
    "# Query user data\n",
    "user_data = supabase.table(\"users\").select(\"*\").execute()\n",
    "users_df = pd.DataFrame(user_data.data)\n",
    "\n",
    "# Query post data\n",
    "post_data = supabase.table(\"posts\").select(\"*\").execute()\n",
    "posts_df = pd.DataFrame(post_data.data)\n",
    "\n",
    "# Query post data\n",
    "collab_data = supabase.table(\"active_collabs\").select(\"title, owner_uid, team_uids\").execute()\n",
    "collabs_df = pd.DataFrame(collab_data.data)\n",
    "\n",
    "#need to assign titles of collabs to all of their corresponding users' profiles in users_df\n",
    "# Ensure 'team_uids' is a list (if it's not already in that format)\n",
    "collabs_df['team_uids'] = collabs_df['team_uids'].apply(lambda x: x if isinstance(x, list) else [x])\n",
    "\n",
    "# Create a dictionary to store all the collabs titles for each user\n",
    "user_collabs = {}\n",
    "\n",
    "# Iterate through each row in collabs_df to map collab titles to users\n",
    "for _, row in collabs_df.iterrows():\n",
    "    collab_title = row['title']\n",
    "    owner_uid = row['owner_uid']\n",
    "    team_uids = row['team_uids']\n",
    "\n",
    "    # Add the collab title to the owner's list of collaborations\n",
    "    if owner_uid not in user_collabs:\n",
    "        user_collabs[owner_uid] = []\n",
    "    user_collabs[owner_uid].append(collab_title)\n",
    "    \n",
    "    # Add the collab title to each team member's list of collaborations\n",
    "    for uid in team_uids:\n",
    "        if uid not in user_collabs:\n",
    "            user_collabs[uid] = []\n",
    "        user_collabs[uid].append(collab_title)\n",
    "\n",
    "# Create a new column in users_df for the collab titles\n",
    "users_df['project_titles'] = users_df['uid'].map(user_collabs)\n",
    "\n",
    "# Fill any NaN values (users without collaborations) with an empty list\n",
    "users_df['project_titles'] = users_df['project_titles'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "# Query interaction data\n",
    "interaction_data = supabase.table(\"user_post_interactions\").select(\"*\").execute()\n",
    "interactions_df = pd.DataFrame(interaction_data.data)\n",
    "\n",
    "# Query post comments data\n",
    "comments_data = supabase.table(\"posts_comments\").select(\"*\").execute()\n",
    "comments_df = pd.DataFrame(comments_data.data)\n",
    "\n",
    "# need to assign posts_df['comments'] with lists of all of the comments that have posts_df['po_id'] in comments_df['po_id']\n",
    "# Group comments by 'po_id' and aggregate them into lists\n",
    "comments_grouped = comments_df.groupby('po_id')['content'].apply(list).to_dict()\n",
    "\n",
    "# Map the comments to the corresponding posts in posts_df based on 'po_id'\n",
    "posts_df['comments'] = posts_df['po_id'].map(comments_grouped)\n",
    "\n",
    "# Fill any NaN values (posts without comments) with an empty list\n",
    "posts_df['comments'] = posts_df['comments'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "# Convert created_at timestamp to hour of day and day of week\n",
    "interactions_df['hour_of_day'] = pd.to_datetime(interactions_df['interaction_time']).dt.hour \n",
    "interactions_df['day_of_week'] = pd.to_datetime(interactions_df['interaction_time']).dt.weekday # 0 = Monday, 6 = Sunday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "id": "7d9ce0f3-97d9-448f-b8c1-423a3f78f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None values in passions, project titles, and bio with empty strings\n",
    "users_df['passions'] = users_df['passions'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '')\n",
    "users_df['project_titles'] = users_df['project_titles'].apply(lambda x: ' '.join(x) if isinstance(x, list) else '')\n",
    "users_df['bio'] = users_df['bio'].apply(lambda x: str(x) if x else '')\n",
    "\n",
    "# Combine all text fields into a single list for fitting the tokenizer\n",
    "all_text = users_df['passions'].tolist() + users_df['project_titles'].tolist() + users_df['bio'].tolist()\n",
    "\n",
    "# Fit tokenizer on all text once\n",
    "tokenizer = Tokenizer(num_words=1000)  # Adjust num_words as needed\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "\n",
    "# Convert passions to sequences\n",
    "passions_sequences = tokenizer.texts_to_sequences(users_df['passions'])\n",
    "passions_padded = pad_sequences(passions_sequences, maxlen=10)  # Adjust maxlen based on your data\n",
    "\n",
    "# Convert project titles to sequences\n",
    "project_titles_sequences = tokenizer.texts_to_sequences(users_df['project_titles'])\n",
    "project_titles_padded = pad_sequences(project_titles_sequences, maxlen=10)\n",
    "\n",
    "# Convert bio to sequences\n",
    "bio_sequences = tokenizer.texts_to_sequences(users_df['bio'])\n",
    "bio_padded = pad_sequences(bio_sequences, maxlen=50)\n",
    "\n",
    "# You can now repeat this process for your posts data if needed\n",
    "all_text = posts_df['title'].tolist() + posts_df['description'].tolist()\n",
    "\n",
    "# Fit tokenizer on all text once\n",
    "tokenizer = Tokenizer(num_words=1000)  # Adjust num_words as needed\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "\n",
    "# Process title and description as text (posts)\n",
    "title_sequences = tokenizer.texts_to_sequences(posts_df['title'])\n",
    "title_padded = pad_sequences(title_sequences, maxlen=10)  # Adjust maxlen for title\n",
    "\n",
    "description_sequences = tokenizer.texts_to_sequences(posts_df['description'])\n",
    "description_padded = pad_sequences(description_sequences, maxlen=100)  # Adjust maxlen for description\n",
    "\n",
    "comments_sequences = tokenizer.texts_to_sequences(posts_df['comments'])\n",
    "comments_padded = pad_sequences(comments_sequences, maxlen=100)  # Adjust maxlen for description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "id": "5da7a9d8-bebd-49eb-8c47-242b6fb381bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "from shapely.wkt import loads as load_wkt\n",
    "from shapely import wkb\n",
    "\n",
    "# Ensure that lat_long is either Point object or default Point\n",
    "users_df['lat_long'] = users_df['lat_long'].apply(lambda wkb_hex: wkb.loads(bytes.fromhex(wkb_hex)) if pd.notnull(wkb_hex) else Point(1, 1))\n",
    "\n",
    "# Now extract latitude and longitude from the Point objects\n",
    "users_df['latitude'] = users_df['lat_long'].apply(lambda loc: loc.y if loc else None)\n",
    "users_df['longitude'] = users_df['lat_long'].apply(lambda loc: loc.x if loc else None)\n",
    "\n",
    "# Finally, convert latitude and longitude to H3 index\n",
    "users_df['h3_location'] = users_df.apply(\n",
    "    lambda row: h3.latlng_to_cell(row['latitude'], row['longitude'], 8) if pd.notnull(row['latitude']) and pd.notnull(row['longitude']) else None,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "id": "8da5e584-1d23-45c9-8606-a738f6b7f27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate post length (number of characters in the description)\n",
    "posts_df['post_length'] = posts_df['description'].apply(len)\n",
    "posts_df['vote_count'] = posts_df['votes'].apply(len)\n",
    "posts_df['upvote_count'] = posts_df['upvotes']-posts_df['downvotes']\n",
    "posts_df['view_count'] = posts_df['views'].apply(len)\n",
    "posts_df['comment_count'] = posts_df['comments'].apply(len)\n",
    "posts_df['avg_time_viewed'] = posts_df['view_lengths'].apply(lambda x: sum(x)/len(x) if len(x) > 0 else 0)\n",
    "\n",
    "\n",
    "# Normalize numerical features (vote count, upvote count, view count, post length, comment count, avg_time_viewed)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "posts_df[['post_length', 'vote_count', 'upvote_count', 'view_count', 'comment_count', 'avg_time_viewed']] = scaler.fit_transform(\n",
    "    posts_df[['post_length', 'vote_count', 'upvote_count', 'view_count', 'comment_count', 'avg_time_viewed']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "id": "0e262b80-79c2-4ef3-8ee7-651e791a3fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count behavioral features (viewed, upvoted, and commented posts) for each user\n",
    "viewed_posts_count = interactions_df[interactions_df['interaction_type'] == 'view'].groupby('user_id')['post_id'].count()\n",
    "upvoted_posts_count = interactions_df[interactions_df['interaction_type'] == 'upvote'].groupby('user_id')['post_id'].count()\n",
    "commented_posts_count = interactions_df[interactions_df['interaction_type'] == 'comment'].groupby('user_id')['post_id'].count()\n",
    "\n",
    "# Map the interaction counts to the corresponding users in users_df\n",
    "users_df['viewed_posts'] = users_df['uid'].map(viewed_posts_count)\n",
    "users_df['upvoted_posts'] = users_df['uid'].map(upvoted_posts_count)\n",
    "users_df['commented_posts'] = users_df['uid'].map(commented_posts_count)\n",
    "\n",
    "# Fill NaN values with 0 for users with no interactions\n",
    "users_df[['viewed_posts', 'upvoted_posts', 'commented_posts']] = users_df[['viewed_posts', 'upvoted_posts', 'commented_posts']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "id": "7b450dc6-3058-4c88-bed9-e0bbecaedf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_negative_samples(user_id, all_post_ids, positive_post_ids, num_negatives=5):\n",
    "    # Get the set of post IDs the user has NOT interacted with\n",
    "    negative_post_ids = list(set(all_post_ids) - set(positive_post_ids))\n",
    "    \n",
    "    # Randomly sample negative examples\n",
    "    sampled_negatives = random.sample(negative_post_ids, min(num_negatives, len(negative_post_ids)))\n",
    "    return sampled_negatives\n",
    "\n",
    "# Generate the dataset with positive and negative samples\n",
    "all_post_ids = posts_df['po_id'].tolist()\n",
    "interaction_dataset_with_negatives = []\n",
    "\n",
    "for user_id, group in interactions_df.groupby('user_id'):\n",
    "    # Collect all post IDs the user has interacted with\n",
    "    positive_post_ids = group['post_id'].tolist()\n",
    "    \n",
    "    # Generate negative samples for the user\n",
    "    negative_samples = generate_negative_samples(user_id, all_post_ids, positive_post_ids)\n",
    "    \n",
    "    # Add positive interactions to the dataset\n",
    "    for _, row in group.iterrows():\n",
    "        interaction_dataset_with_negatives.append({\n",
    "            'user_id': row['user_id'],\n",
    "            'post_id': row['post_id'],\n",
    "            'interaction_type': row['interaction_type'],  # \"view\", \"comment\", \"upvote\"\n",
    "            'view_duration_secs': row['view_duration_secs'],\n",
    "            'comment_length': row['comment_length'],\n",
    "            'hour_of_day': row['hour_of_day'], \n",
    "            'day_of_week': row['day_of_week']\n",
    "        })\n",
    "    \n",
    "    # Add negative samples to the dataset\n",
    "    for post_id in negative_samples:\n",
    "        interaction_dataset_with_negatives.append({\n",
    "            'user_id': user_id,\n",
    "            'post_id': post_id,\n",
    "            'interaction_type': 'negative',  # Label as \"negative\"\n",
    "            'view_duration_secs': 0,  # No view duration for negative samples\n",
    "            'comment_length': 0,  # No comment length for negative samples\n",
    "            'hour_of_day': None, # No hour_of_day for negative samples \n",
    "            'day_of_week': None # No day_of_week for negative samples\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "id": "d02f1a05-e854-4b3c-be8e-a3b5cb2b394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract additional interaction features such as view duration and comment length\n",
    "interaction_features = interactions_df.copy()\n",
    "\n",
    "# Calculate comment length (assuming 'comment_text' contains the comment content)\n",
    "interaction_features['comment_length'] = interaction_features['comment_length'].apply(lambda x: x if pd.notnull(x) else 0)\n",
    "\n",
    "# Fill NaN values with 0 for interaction-specific features\n",
    "interaction_features[['view_duration_secs', 'comment_length']] = interaction_features[['view_duration_secs', 'comment_length']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "id": "e4d672fc-f5a4-420c-bc97-16ff69be73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrames to TensorFlow Datasets for users\n",
    "user_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    \"user_id\": users_df[\"uid\"].values,\n",
    "    \"passions\": passions_padded,  \n",
    "    \"project_titles\": project_titles_padded,  # Add project titles\n",
    "    \"bio\": bio_padded,  # Add bio\n",
    "    \"h3_location\": users_df[\"h3_location\"].values,  \n",
    "    \"created_at\": users_df[\"created_at\"].values,  \n",
    "    \"viewed_posts\": users_df[\"viewed_posts\"].values,  # Behavioral data\n",
    "    \"upvoted_posts\": users_df[\"upvoted_posts\"].values,\n",
    "    \"commented_posts\": users_df[\"commented_posts\"].values,\n",
    "})\n",
    "\n",
    "# Convert DataFrames to TensorFlow Datasets for posts\n",
    "post_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    \"post_id\": posts_df[\"po_id\"].values,\n",
    "    #\"title\": posts_df[\"title\"].values,\n",
    "    #\"description\": posts_df[\"description\"].values,\n",
    "    \"title\": title_padded,\n",
    "    \"description\": description_padded,\n",
    "    \"post_length\": posts_df[\"post_length\"].values,  # Add post length\n",
    "    \"vote_count\": posts_df[\"vote_count\"].values,\n",
    "    \"upvote_count\": posts_df[\"upvote_count\"].values,\n",
    "    \"view_count\": posts_df[\"view_count\"].values,\n",
    "    \"comment_count\": posts_df[\"comment_count\"].values,  # Add comment count\n",
    "    \"comments\": comments_padded,  # Add comments\n",
    "    \"avg_time_viewed\": posts_df[\"avg_time_viewed\"].values,  # Add avg time viewed\n",
    "})\n",
    "\n",
    "# Convert the interaction dataset (with negative samples) into TensorFlow Dataset\n",
    "interaction_df = pd.DataFrame(interaction_dataset_with_negatives)\n",
    "interaction_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    \"user_id\": interaction_df[\"user_id\"].values,\n",
    "    \"post_id\": interaction_df[\"post_id\"].values,\n",
    "    \"interaction_type\": interaction_df[\"interaction_type\"].values,#view,comment,or upvote\n",
    "    \"view_duration_secs\": interaction_df[\"view_duration_secs\"].values, #error is caused by this line even though there are no nonetypes in it\n",
    "    \"comment_length\": interaction_df[\"comment_length\"].values,\n",
    "    \"hour_of_day\": interaction_df[\"hour_of_day\"].values, # Add hour of day \n",
    "    \"day_of_week\": interaction_df[\"day_of_week\"].values, # Add day of week\n",
    "})\n",
    "#list(interaction_dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "id": "f1d63db3-1509-4510-9b73-2cc932599d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserTower(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.passion_embedding = tf.keras.layers.Embedding(input_dim=5000, output_dim=64)\n",
    "        self.project_title_embedding = tf.keras.layers.Embedding(input_dim=5000, output_dim=64)\n",
    "        self.bio_embedding = tf.keras.layers.Embedding(input_dim=5000, output_dim=64)\n",
    "\n",
    "        # Embeddings for hour_of_day and day_of_week\n",
    "        self.hour_of_day_embedding = tf.keras.layers.Embedding(input_dim=24, output_dim=8)  # 24 hours in a day\n",
    "        self.day_of_week_embedding = tf.keras.layers.Embedding(input_dim=7, output_dim=8)   # 7 days in a week\n",
    "        self.created_at_embedding = tf.keras.layers.Embedding(input_dim=5000, output_dim=64)   # 7 days in a week\n",
    "\n",
    "        # Behavioral data layers\n",
    "        self.behavioral_dense = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(16)\n",
    "        ])\n",
    "        \n",
    "        self.location_dense = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(16)\n",
    "        ])\n",
    "        \n",
    "        self.final_dense = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(32)\n",
    "        ])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Build embeddings and numerical_dense based on individual input features\n",
    "        self.passion_embedding.build(input_shape['passions'])\n",
    "        self.project_title_embedding.build(input_shape['project_titles'])\n",
    "        self.bio_embedding.build(input_shape['bio'])\n",
    "\n",
    "        # Create an input shape for the concatenated numerical features (since these are individually expanded)\n",
    "        behavioral_input_shape = (input_shape['viewed_posts'][0], 3)  # Assuming 6 numerical features\n",
    "        self.behavioral_dense.build(behavioral_input_shape)\n",
    "\n",
    "        location_input_shape = (input_shape['h3_location'][0], 1)  # Assuming 6 numerical features\n",
    "        self.location_dense.build(location_input_shape)\n",
    "\n",
    "        super(UserTower, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        passion_embedding = self.passion_embedding(inputs[\"passions\"])\n",
    "        project_title_embedding = self.project_title_embedding(inputs[\"project_titles\"])\n",
    "        bio_embedding = self.bio_embedding(inputs[\"bio\"])\n",
    "    \n",
    "\t    # Embed hour_of_day and day_of_week \n",
    "        #hour_of_day_embedding = self.hour_of_day_embedding(inputs[\"hour_of_day\"]) \n",
    "        #day_of_week_embedding = self.day_of_week_embedding(inputs[\"day_of_week\"])\n",
    "        created_at_embedding = self.created_at_embedding(inputs[\"created_at\"]) \n",
    "        created_at_embedding = tf.expand_dims(created_at_embedding, axis=1)  # Expand to Shape: [?, 1, 64]\n",
    "        \n",
    "        # Combine embeddings for behavioral features\n",
    "        behavioral_features = tf.concat([\n",
    "            tf.expand_dims(inputs[\"viewed_posts\"], axis=1), \n",
    "            tf.expand_dims(inputs[\"upvoted_posts\"], axis=1), \n",
    "            tf.expand_dims(inputs[\"commented_posts\"], axis=1)\n",
    "        ], axis=1)\n",
    "        behavioral_embedding = self.behavioral_dense(behavioral_features)  # Shape: [?, 16]\n",
    "        behavioral_embedding = tf.keras.layers.Dense(64)(behavioral_embedding)  # Project to Shape: [?, 64]\n",
    "        behavioral_embedding = tf.expand_dims(behavioral_embedding, axis=1)  # Expand to Shape: [?, 1, 64]\n",
    "    \n",
    "        # Embed location\n",
    "        location_embedding = self.location_dense(tf.expand_dims(inputs[\"h3_location\"], -1))  # Shape: [?, 16]\n",
    "        location_embedding = tf.keras.layers.Dense(64)(location_embedding)  # Project to Shape: [?, 64]\n",
    "        location_embedding = tf.expand_dims(location_embedding, axis=1)  # Expand to Shape: [?, 1, 64]\n",
    "\n",
    "        combined = tf.concat([passion_embedding, project_title_embedding, bio_embedding, behavioral_embedding, location_embedding, created_at_embedding], axis=1)\n",
    "        \n",
    "        # A dense layer after concatenation to map features to a unified space\n",
    "        combined_dense = tf.keras.layers.Dense(128, activation=\"relu\")(combined)\n",
    "        \n",
    "        output = self.final_dense(combined_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "id": "597fd0ef-36df-4677-9fd2-7599f4a4c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostTower(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.title_embedding = tf.keras.layers.Embedding(input_dim=5000, output_dim=64)\n",
    "        self.description_embedding = tf.keras.layers.Embedding(input_dim=5000, output_dim=64)\n",
    "        self.comment_embedding = tf.keras.layers.Embedding(input_dim=5000, output_dim=64)  # Embedding for comments\n",
    "\n",
    "        # Process numerical features (including new post length, comment count, etc.)\n",
    "        self.numerical_dense = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),  # Project numerical features to 64 dimensions\n",
    "            tf.keras.layers.Dense(64)  # Ensure it matches the embedding dimensions\n",
    "        ])\n",
    "\n",
    "        self.final_dense = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(32)\n",
    "        ])\n",
    "\n",
    "    def aggregate_comments(self, post_text, comments):\n",
    "        post_embedding = self.description_embedding(post_text)\n",
    "    \n",
    "        # Use tf.map_fn to apply the embedding function over the comments tensor\n",
    "        comment_embeddings = tf.map_fn(self.comment_embedding, comments, fn_output_signature=tf.float32)\n",
    "        \n",
    "        post_weight = 0.7\n",
    "        comment_weight = 0.3\n",
    "        \n",
    "        # Combine the post and comment embeddings (using weights)\n",
    "        combined_embedding = post_weight * post_embedding + comment_weight * tf.reduce_sum(comment_embeddings, axis=0)\n",
    "        \n",
    "        return combined_embedding    \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Build embeddings and numerical_dense based on individual input features\n",
    "        self.title_embedding.build(input_shape['title'])\n",
    "        self.description_embedding.build(input_shape['description'])\n",
    "\n",
    "        # Create an input shape for the concatenated numerical features (since these are individually expanded)\n",
    "        numerical_input_shape = (input_shape['post_length'][0], 6)  # Assuming 6 numerical features\n",
    "        self.numerical_dense.build(numerical_input_shape)\n",
    "\n",
    "        super(PostTower, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Embed the title and description\n",
    "        title_embedding = self.title_embedding(inputs[\"title\"])  # Shape: (batch_size, None, 10, 64)\n",
    "        description_embedding = self.description_embedding(inputs[\"description\"])  # Shape: (batch_size, None, 100, 64)\n",
    "        \n",
    "        # Embed the post content and comments (ensure rank is consistent)\n",
    "        combined_post_embedding = self.aggregate_comments(inputs[\"description\"], inputs[\"comments\"])  # Shape: (batch_size, 64)\n",
    "\n",
    "        numerical_features = tf.concat([\n",
    "            tf.expand_dims(inputs[\"post_length\"], axis=1),\n",
    "            tf.expand_dims(inputs[\"vote_count\"], axis=1),\n",
    "            tf.expand_dims(inputs[\"upvote_count\"], axis=1),\n",
    "            tf.expand_dims(inputs[\"view_count\"], axis=1),\n",
    "            tf.expand_dims(inputs[\"comment_count\"], axis=1),\n",
    "            tf.expand_dims(inputs[\"avg_time_viewed\"], axis=1)\n",
    "        ], axis=1)\n",
    "\n",
    "        \n",
    "        # Project numerical features to match the embedding dimensions (64)\n",
    "        numerical_embedding = self.numerical_dense(numerical_features)  # Shape: (batch_size, num_numerical_features, 64)\n",
    "        # Expand numerical_embedding to match the rank of the other tensors\n",
    "        numerical_embedding = tf.expand_dims(numerical_embedding, axis=1)  # Shape: [batch_size, 1, 64]\n",
    "\n",
    "\n",
    "        combined = tf.concat([title_embedding, combined_post_embedding, numerical_embedding], axis=1)\n",
    "        \n",
    "        return self.final_dense(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "id": "46e01950-d4e2-45f6-bfd6-f42b6d7eb96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "class TwoTowerModel(tfrs.models.Model):\n",
    "    def __init__(self, user_model, post_model):\n",
    "        super().__init__()\n",
    "        self.user_model = user_model\n",
    "        self.post_model = post_model\n",
    "        self.user_dataset = user_dataset  # Store the user dataset\n",
    "        self.post_dataset = post_dataset  # Store the post dataset\n",
    "\n",
    "        self.candidate_index = tfrs.layers.factorized_top_k.BruteForce(post_model)\n",
    "        self.task = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(candidates=self.candidate_index))\n",
    "        \n",
    "        '''\n",
    "        # Map post_dataset to post embeddings for FactorizedTopK metric\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=post_dataset.batch(1).map(lambda post: (\n",
    "                    post[\"post_id\"],\n",
    "                    post_model({\n",
    "                        \"title\": post[\"title\"],\n",
    "                        \"description\": post[\"description\"],\n",
    "                        \"vote_count\": post[\"vote_count\"],\n",
    "                        \"upvote_count\": post[\"upvote_count\"],\n",
    "                        \"view_count\": post[\"view_count\"],\n",
    "                        \"comments\": post[\"comments\"],\n",
    "                        \"post_length\": post[\"post_length\"],\n",
    "                        \"comment_count\": post[\"comment_count\"],\n",
    "                        \"avg_time_viewed\": post[\"avg_time_viewed\"]\n",
    "                    })\n",
    "                ))\n",
    "            )\n",
    "        )\n",
    "        '''\n",
    "\n",
    "    def get_user_features(self, user_ids):\n",
    "        # This function retrieves the corresponding user features (passions, h3_location) based on user_id\n",
    "        user_features = self.user_dataset.filter(lambda user: tf.reduce_any(user[\"user_id\"] == user_ids))\n",
    "        \n",
    "        # Use `get_single_element` to retrieve tensors from the dataset\n",
    "        user_features = user_features.batch(1)  # Ensure batching\n",
    "        user_tensors = tf.compat.v1.data.experimental.get_single_element(user_features)\n",
    "\n",
    "        user_passions = user_tensors[\"passions\"]\n",
    "        user_h3_location = user_tensors[\"h3_location\"]\n",
    "        user_project_titles = user_tensors[\"project_titles\"]\n",
    "        user_bio = user_tensors[\"bio\"]\n",
    "        user_created_at = user_tensors[\"created_at\"]\n",
    "        user_viewed_posts = user_tensors[\"viewed_posts\"]\n",
    "        user_upvoted_posts = user_tensors[\"upvoted_posts\"]\n",
    "        user_commented_posts = user_tensors[\"commented_posts\"]\n",
    "        \n",
    "        return user_passions, user_h3_location, user_project_titles, user_bio, user_created_at, user_viewed_posts, user_upvoted_posts, user_commented_posts\n",
    "\n",
    "    def get_post_features(self, post_ids):\n",
    "        # This function retrieves the corresponding post features based on post_id\n",
    "        post_features = self.post_dataset.filter(lambda post: tf.reduce_any(post[\"post_id\"] == post_ids))\n",
    "        \n",
    "        # Use `get_single_element` to retrieve tensors from the dataset\n",
    "        post_features = post_features.batch(1)  # Ensure batching\n",
    "        post_tensors = tf.compat.v1.data.experimental.get_single_element(post_features)\n",
    "\n",
    "        post_title = post_tensors[\"title\"]\n",
    "        post_description = post_tensors[\"description\"]\n",
    "        post_vote_count = post_tensors[\"vote_count\"]\n",
    "        post_upvote_count = post_tensors[\"upvote_count\"]\n",
    "        post_view_count = post_tensors[\"view_count\"]\n",
    "        post_comments = post_tensors[\"comments\"]\n",
    "        post_length = post_tensors[\"post_length\"]\n",
    "        post_comment_count = post_tensors[\"comment_count\"]\n",
    "        post_avg_time_viewed = post_tensors[\"avg_time_viewed\"]\n",
    "        \n",
    "        return post_title, post_description, post_vote_count, post_upvote_count, post_view_count, post_comments, post_length, post_comment_count, post_avg_time_viewed\n",
    "        \n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "\n",
    "        # Extract IDs from interaction_dataset\n",
    "        user_ids = features[\"user_id\"]\n",
    "        post_ids = features[\"post_id\"]\n",
    "\n",
    "        # Get user/post-specific features using the ids\n",
    "        user_passions, user_h3_location, user_project_titles, user_bio, user_created_at, user_viewed_posts, user_upvoted_posts, user_commented_posts = self.get_user_features(user_ids)\n",
    "        post_title, post_description, post_vote_count, post_upvote_count, post_view_count, post_comments, post_length, post_comment_count, post_avg_time_viewed = self.get_post_features(post_ids)\n",
    "        \n",
    "        # Compute user embeddings\n",
    "        user_embeddings = self.user_model({\n",
    "            \"passions\": user_passions,\n",
    "            \"h3_location\": user_h3_location,\n",
    "            \"project_titles\": user_project_titles,\n",
    "            \"bio\": user_bio,\n",
    "            \"created_at\": user_created_at,\n",
    "            \"viewed_posts\": user_viewed_posts,\n",
    "            \"upvoted_posts\": user_upvoted_posts,\n",
    "            \"commented_posts\": user_commented_posts,\n",
    "        })\n",
    "\n",
    "        # Compute post embeddings\n",
    "        post_embeddings = self.post_model({\n",
    "            \"title\": post_title,\n",
    "            \"description\": post_description,\n",
    "            \"vote_count\": post_vote_count,\n",
    "            \"upvote_count\": post_upvote_count,\n",
    "            \"view_count\": post_view_count,\n",
    "            \"comments\": post_comments,\n",
    "            \"post_length\": post_length,\n",
    "            \"comment_count\": post_comment_count,\n",
    "            \"avg_time_viewed\": post_avg_time_viewed\n",
    "        })\n",
    "\n",
    "        # Define weights for each interaction type\n",
    "        interaction_weights = {\n",
    "            \"view\": 0.1,\n",
    "            \"comment\": 0.5,\n",
    "            \"upvote\": 0.4\n",
    "        }\n",
    "\n",
    "        # Apply the weight based on the interaction type\n",
    "        interaction_type = features['interaction_type']\n",
    "        interaction_weight = tf.where(\n",
    "            interaction_type == 'view', interaction_weights['view'],\n",
    "            tf.where(interaction_type == 'comment', interaction_weights['comment'], interaction_weights['upvote'])\n",
    "        )\n",
    "\n",
    "        # Positive interaction mask\n",
    "        positive_interaction_mask = tf.cast(\n",
    "            tf.logical_or(tf.logical_or(interaction_type == 'view', interaction_type == 'comment'),\n",
    "                          interaction_type == 'upvote'),\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        # Negative interaction mask\n",
    "        negative_interaction_mask = tf.cast(interaction_type == 'negative', dtype=tf.float32)\n",
    "\n",
    "        # Compute positive and negative scores\n",
    "        positive_scores = positive_interaction_mask * tf.reduce_sum(user_embeddings * post_embeddings, axis=1) * interaction_weight\n",
    "        negative_scores = negative_interaction_mask * tf.reduce_sum(user_embeddings * post_embeddings, axis=1)\n",
    "\n",
    "        # Loss function: Maximize positive scores, minimize negative scores\n",
    "        loss = self.task(user_embeddings, post_embeddings)\n",
    "        return loss - tf.reduce_mean(positive_scores) + tf.reduce_mean(negative_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "id": "c9aec87e-5ffc-4cab-a136-4b55fb0a627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical features\n",
    "normalized_view_count = tf.keras.layers.BatchNormalization()(tf.expand_dims(posts_df[\"view_count\"], -1))\n",
    "normalized_vote_count = tf.keras.layers.BatchNormalization()(tf.expand_dims(posts_df[\"vote_count\"], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "id": "3d78d806-d2ef-4def-a7b9-7962e1c57d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Tensor(\"DatasetToSingleElement:4\", shape=(None, 10), dtype=int32) Tensor(\"DatasetToSingleElement:3\", shape=(None,), dtype=string) Tensor(\"DatasetToSingleElement:5\", shape=(None, 10), dtype=int32) Tensor(\"DatasetToSingleElement:0\", shape=(None, 50), dtype=int32) Tensor(\"DatasetToSingleElement:2\", shape=(None,), dtype=string) Tensor(\"DatasetToSingleElement:8\", shape=(None,), dtype=float64) Tensor(\"DatasetToSingleElement:6\", shape=(None,), dtype=float64) Tensor(\"DatasetToSingleElement:1\", shape=(None,), dtype=float64)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[979], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m cached_train \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m10000\u001b[39m)\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m1024\u001b[39m)\u001b[38;5;241m.\u001b[39mcache()\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv310/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/myenv310/lib/python3.10/site-packages/tensorflow_recommenders/models/base.py:68\u001b[0m, in \u001b[0;36mModel.train_step\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom train step using the `compute_loss` method.\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 68\u001b[0m   loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m   \u001b[38;5;66;03m# Handle regularization losses as well.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m   regularization_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses)\n",
      "Cell \u001b[0;32mIn[977], line 136\u001b[0m, in \u001b[0;36mTwoTowerModel.compute_loss\u001b[0;34m(self, features, training)\u001b[0m\n\u001b[1;32m    133\u001b[0m negative_interaction_mask \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(interaction_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Compute positive and negative scores\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m positive_scores \u001b[38;5;241m=\u001b[39m positive_interaction_mask \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(\u001b[43muser_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpost_embeddings\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m interaction_weight\n\u001b[1;32m    137\u001b[0m negative_scores \u001b[38;5;241m=\u001b[39m negative_interaction_mask \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(user_embeddings \u001b[38;5;241m*\u001b[39m post_embeddings, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Loss function: Maximize positive scores, minimize negative scores\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported."
     ]
    }
   ],
   "source": [
    "# Split the dataset for training and validation\n",
    "train_dataset = interaction_dataset.take(36)\n",
    "test_dataset = interaction_dataset.skip(36)\n",
    "\n",
    "user_model = UserTower()\n",
    "post_model = PostTower()\n",
    "model = TwoTowerModel(user_model, post_model)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "\n",
    "# Train the model\n",
    "cached_train = train_dataset.shuffle(10000).batch(1024).cache()\n",
    "model.fit(cached_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405dce93-4204-4e74-bb15-49c45373343d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb3948-c829-4072-8e3c-8073f2587da3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
